"""
Script para ejecutar EDA completo con dataset reducido

Ejecuta el notebook EDA_Suspicious_Login_RBA.ipynb con el dataset reducido
y genera anÃ¡lisis estadÃ­stico completo.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# ConfiguraciÃ³n
DATASET = "rba_reduced.csv"

print("=" * 80)
print("   EDA COMPLETO - Dataset Reducido (<100K)")
print("=" * 80)
print(f"\nðŸ“… Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print(f"ðŸ“ Dataset: {DATASET}")

# Cargar dataset
df = pd.read_csv(DATASET)
df['Login Timestamp'] = pd.to_datetime(df['Login Timestamp'])

print(f"\nâœ… Dataset cargado: {len(df):,} registros")

# ============================================================================
# FASE 1: ANÃLISIS GENERAL
# ============================================================================
print("\n" + "=" * 80)
print("FASE 1: ANÃLISIS GENERAL DEL DATASET")
print("=" * 80)

print(f"\nðŸ“Š INFORMACIÃ“N DEL DATASET:")
print(f"   â€¢ Total registros: {len(df):,}")
print(f"   â€¢ Total columnas: {len(df.columns)}")
print(f"   â€¢ TamaÃ±o en memoria: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")

print(f"\nðŸ“Š COLUMNAS:")
for col in df.columns:
    dtype = df[col].dtype
    nulls = df[col].isnull().sum()
    null_pct = (nulls / len(df)) * 100
    print(f"   â€¢ {col:30s}: {str(dtype):10s} | Nulls: {nulls:,} ({null_pct:.2f}%)")

# ============================================================================
# FASE 2: TARGET VARIABLE - Is Account Takeover
# ============================================================================
print("\n" + "=" * 80)
print("FASE 2: ANÃLISIS DE TARGET VARIABLE")
print("=" * 80)

ato_count = df['Is Account Takeover'].sum()
normal_count = len(df) - ato_count

print(f"\nðŸ“Š DISTRIBUCIÃ“N DE Is Account Takeover:")
print(f"   â€¢ Account Takeover: {ato_count} ({ato_count/len(df)*100:.4f}%)")
print(f"   â€¢ Normal: {normal_count:,} ({normal_count/len(df)*100:.4f}%)")
print(f"   â€¢ Ratio: {normal_count/ato_count:.1f}:1 (Normal:ATO)")

# ============================================================================
# FASE 3: LOGIN SUCCESS/FAILURE
# ============================================================================
print("\n" + "=" * 80)
print("FASE 3: ANÃLISIS DE LOGIN SUCCESS/FAILURE")
print("=" * 80)

success_count = df['Login Successful'].sum()
failure_count = len(df) - success_count

print(f"\nðŸ“Š DISTRIBUCIÃ“N DE Login Successful:")
print(f"   â€¢ Logins EXITOSOS: {success_count:,} ({success_count/len(df)*100:.2f}%)")
print(f"   â€¢ Logins FALLIDOS: {failure_count:,} ({failure_count/len(df)*100:.2f}%)")

# RelaciÃ³n con ATOs
ato_df = df[df['Is Account Takeover'] == True]
normal_df = df[df['Is Account Takeover'] == False]

ato_success = ato_df['Login Successful'].sum()
normal_success = normal_df['Login Successful'].sum()

print(f"\nðŸ“Š LOGIN SUCCESS EN ATOs:")
print(f"   â€¢ Exitosos: {ato_success} ({ato_success/len(ato_df)*100:.1f}%)")
print(f"   â€¢ Fallidos: {len(ato_df) - ato_success} ({(len(ato_df) - ato_success)/len(ato_df)*100:.1f}%)")

print(f"\nðŸ“Š LOGIN SUCCESS EN NORMALES:")
print(f"   â€¢ Exitosos: {normal_success:,} ({normal_success/len(normal_df)*100:.2f}%)")
print(f"   â€¢ Fallidos: {len(normal_df) - normal_success:,} ({(len(normal_df) - normal_success)/len(normal_df)*100:.2f}%)")

# ============================================================================
# FASE 4: FEATURES CATEGÃ“RICOS
# ============================================================================
print("\n" + "=" * 80)
print("FASE 4: ANÃLISIS DE FEATURES CATEGÃ“RICOS")
print("=" * 80)

# Browser
print(f"\nðŸ“Š BROWSERS (Top 10):")
top_browsers = df['Browser Name and Version'].value_counts().head(10)
for browser, count in top_browsers.items():
    pct = count / len(df) * 100
    print(f"   {browser}: {count:,} ({pct:.2f}%)")

# OS
print(f"\nðŸ“Š SISTEMAS OPERATIVOS (Top 10):")
top_os = df['OS Name and Version'].value_counts().head(10)
for os, count in top_os.items():
    pct = count / len(df) * 100
    print(f"   {os}: {count:,} ({pct:.2f}%)")

# Device Type
print(f"\nðŸ“Š DEVICE TYPE:")
device_counts = df['Device Type'].value_counts()
for device, count in device_counts.items():
    pct = count / len(df) * 100
    print(f"   {device}: {count:,} ({pct:.2f}%)")

# ============================================================================
# FASE 5: FEATURES NUMÃ‰RICOS
# ============================================================================
print("\n" + "=" * 80)
print("FASE 5: ANÃLISIS DE FEATURES NUMÃ‰RICOS")
print("=" * 80)

# Round-Trip Time
print(f"\nðŸ“Š ROUND-TRIP TIME [ms]:")
rtt_stats = df['Round-Trip Time [ms]'].describe()
print(f"   â€¢ Media: {rtt_stats['mean']:.1f}ms")
print(f"   â€¢ Mediana: {rtt_stats['50%']:.1f}ms")
print(f"   â€¢ Std Dev: {rtt_stats['std']:.1f}ms")
print(f"   â€¢ Min: {rtt_stats['min']:.1f}ms")
print(f"   â€¢ Max: {rtt_stats['max']:.1f}ms")

# ASN
print(f"\nðŸ“Š ASN (Autonomous System Number):")
asn_unique = df['ASN'].nunique()
print(f"   â€¢ ASNs Ãºnicos: {asn_unique:,}")
print(f"   â€¢ Top 10 ASNs:")
top_asn = df['ASN'].value_counts().head(10)
for asn, count in top_asn.items():
    pct = count / len(df) * 100
    print(f"      ASN {asn}: {count:,} ({pct:.2f}%)")

# ============================================================================
# FASE 6: ANÃLISIS TEMPORAL
# ============================================================================
print("\n" + "=" * 80)
print("FASE 6: ANÃLISIS TEMPORAL")
print("=" * 80)

# Extraer componentes temporales
df['hour'] = df['Login Timestamp'].dt.hour
df['day_of_week'] = df['Login Timestamp'].dt.dayofweek
df['day_name'] = df['Login Timestamp'].dt.day_name()

print(f"\nðŸ“Š PERIODO DEL DATASET:")
print(f"   â€¢ Inicio: {df['Login Timestamp'].min()}")
print(f"   â€¢ Fin: {df['Login Timestamp'].max()}")
print(f"   â€¢ DuraciÃ³n: {(df['Login Timestamp'].max() - df['Login Timestamp'].min()).days} dÃ­as")

print(f"\nðŸ“Š DISTRIBUCIÃ“N POR HORA DEL DÃA (Top 10):")
hour_counts = df['hour'].value_counts().sort_index().head(10)
for hour, count in hour_counts.items():
    pct = count / len(df) * 100
    print(f"   Hora {hour:02d}:00: {count:,} ({pct:.2f}%)")

print(f"\nðŸ“Š DISTRIBUCIÃ“N POR DÃA DE LA SEMANA:")
day_counts = df['day_name'].value_counts()
days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
for day in days_order:
    if day in day_counts:
        count = day_counts[day]
        pct = count / len(df) * 100
        print(f"   {day:9s}: {count:,} ({pct:.2f}%)")

# ============================================================================
# FASE 7: IS ATTACK IP
# ============================================================================
print("\n" + "=" * 80)
print("FASE 7: ANÃLISIS DE ATTACK IPs")
print("=" * 80)

attack_ip_count = df['Is Attack IP'].sum()
print(f"\nðŸ“Š IPs DE ATAQUE:")
print(f"   â€¢ IPs marcadas como ataque: {attack_ip_count:,} ({attack_ip_count/len(df)*100:.2f}%)")
print(f"   â€¢ IPs normales: {len(df) - attack_ip_count:,} ({(len(df) - attack_ip_count)/len(df)*100:.2f}%)")

# RelaciÃ³n con ATOs
ato_with_attack_ip = ato_df['Is Attack IP'].sum()
print(f"\nðŸ“Š ATTACK IP EN ATOs:")
print(f"   â€¢ ATOs con Attack IP: {ato_with_attack_ip} ({ato_with_attack_ip/len(ato_df)*100:.1f}%)")

# ============================================================================
# FASE 8: USUARIOS E IPs
# ============================================================================
print("\n" + "=" * 80)
print("FASE 8: ANÃLISIS DE USUARIOS E IPs")
print("=" * 80)

users_unique = df['User ID'].nunique()
ips_unique = df['IP Address'].nunique()

print(f"\nðŸ“Š USUARIOS E IPs:")
print(f"   â€¢ Usuarios Ãºnicos: {users_unique:,}")
print(f"   â€¢ IPs Ãºnicas: {ips_unique:,}")
print(f"   â€¢ Ratio IP/Usuario: {ips_unique/users_unique:.2f}")

# Usuarios con mÃºltiples IPs
users_ip_counts = df.groupby('User ID')['IP Address'].nunique()
multi_ip_users = users_ip_counts[users_ip_counts > 1]

print(f"\nðŸ“Š USUARIOS CON MÃšLTIPLES IPs:")
print(f"   â€¢ Usuarios con 1 IP: {(users_ip_counts == 1).sum():,}")
print(f"   â€¢ Usuarios con 2+ IPs: {len(multi_ip_users):,}")
print(f"   â€¢ Usuarios con 5+ IPs: {(users_ip_counts >= 5).sum():,}")
print(f"   â€¢ Usuarios con 10+ IPs: {(users_ip_counts >= 10).sum():,}")

# IPs con mÃºltiples usuarios
ips_user_counts = df.groupby('IP Address')['User ID'].nunique()
multi_user_ips = ips_user_counts[ips_user_counts > 1]

print(f"\nðŸ“Š IPs CON MÃšLTIPLES USUARIOS:")
print(f"   â€¢ IPs con 1 usuario: {(ips_user_counts == 1).sum():,}")
print(f"   â€¢ IPs con 2+ usuarios: {len(multi_user_ips):,}")
print(f"   â€¢ IPs con 5+ usuarios: {(ips_user_counts >= 5).sum():,}")
print(f"   â€¢ IPs con 10+ usuarios: {(ips_user_counts >= 10).sum():,}")

# ============================================================================
# FASE 9: CONCLUSIONES
# ============================================================================
print("\n" + "=" * 80)
print("FASE 9: CONCLUSIONES DEL EDA")
print("=" * 80)

print(f"\nðŸŽ¯ HALLAZGOS PRINCIPALES:")

print(f"\n1. BALANCE DE CLASES:")
print(f"   â€¢ Dataset EXTREMADAMENTE DESBALANCEADO")
print(f"   â€¢ Ratio 1:{normal_count/ato_count:.0f} (Normal:ATO)")
print(f"   â€¢ Requiere class_weight='balanced' o SMOTE")

print(f"\n2. PATRÃ“N DE ATOs:")
print(f"   â€¢ {ato_success/len(ato_df)*100:.1f}% de ATOs son logins EXITOSOS")
print(f"   â€¢ Los atacantes YA TIENEN credenciales vÃ¡lidas")
print(f"   â€¢ NO es detecciÃ³n de fuerza bruta")
print(f"   â€¢ DetecciÃ³n basada en: IP nueva, paÃ­s diferente, dispositivo diferente")

print(f"\n3. CASOS NORMALES:")
print(f"   â€¢ {(len(normal_df) - normal_success)/len(normal_df)*100:.2f}% de logins normales FALLAN")
print(f"   â€¢ Esto es comportamiento humano esperado (typos, olvidos)")
print(f"   â€¢ NO son ataques, son usuarios legÃ­timos")

print(f"\n4. FEATURES ÃšTILES:")
print(f"   â€¢ Country (cambio de paÃ­s en ATOs)")
print(f"   â€¢ IP Address (IPs nuevas)")
print(f"   â€¢ Browser/OS/Device (cambios de dispositivo)")
print(f"   â€¢ Is Attack IP (correlaciÃ³n con ATOs)")
print(f"   â€¢ Round-Trip Time (latencia anÃ³mala)")
print(f"   â€¢ Login Timestamp (patrones temporales)")

print(f"\n5. MÃ‰TRICAS RECOMENDADAS:")
print(f"   â€¢ Recall (PRIORIDAD - detectar todos los ATOs)")
print(f"   â€¢ F1-Score (balance Precision/Recall)")
print(f"   â€¢ AUC-PR (mejor que ROC-AUC para desbalance)")
print(f"   â€¢ NO usar Accuracy (inÃºtil con desbalance extremo)")

print("\n" + "=" * 80)
print("âœ… EDA COMPLETADO")
print("=" * 80)

print(f"\nðŸ“‹ RESUMEN:")
print(f"   â€¢ Dataset: {DATASET}")
print(f"   â€¢ Registros: {len(df):,}")
print(f"   â€¢ ATOs: {ato_count}")
print(f"   â€¢ Features: {len(df.columns)}")

print(f"\nðŸŽ¯ PRÃ“XIMOS PASOS:")
print(f"   1. Feature Engineering (35 features)")
print(f"   2. Temporal Split (no random)")
print(f"   3. Entrenar modelos con class_weight='balanced'")
print(f"   4. Threshold tuning para maximizar Recall")

print("\n" + "=" * 80)
